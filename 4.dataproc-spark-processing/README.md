# ğŸŒ¤ï¸ Project 4/50: Batch Processing with PySpark, Dataproc & BigQuery External table on GCP

Process historical order data using PySpark on Dataproc, write partitioned Parquet files to GCS, and create a BigQuery External table on top â€” ready for reporting!

---

![flow-diagram](gcp-project.gif)

## ğŸ› ï¸ Tools Used
- **Google Cloud Storage (GCS)** â€“ for raw and processed data
- **Dataproc (PySpark)** â€“ to clean and transform data
- **BigQuery External Table** â€“ external table on top of partitioned GCS data
- **Looker Studio** â€“ for interactive dashboards

---

## ğŸ“ Project Structure

```
4.dataproc-spark-processing/
â”œâ”€â”€ main.py                   # Cloud Function to fetch and write data
â”œâ”€â”€ data                      # contains the input raw orders_data.csv
â”œâ”€â”€ setup.sh                  # create GCS bucket, BQ dataset, load raw data
â”œâ”€â”€ run_dataproc.py           # create dataproc cluster & submit the Dataproc job
â””â”€â”€ README.md
```

---

## ğŸš€ How It Works

1. Uploads `order_data.csv` to GCS.
2. Submits a PySpark job to Dataproc.
   - Reads raw CSV
   - Cleans data
   - Extracts `partition_date` from `OrderDate`
   - Writes partitioned Parquet to GCS
3. Creates a **BigLake external table** pointing to GCS using Hive-style partitioning.
4. Output is queryable in BigQuery.
5. Optional: Visualize using Looker Studio.

---

## âœ… Setup

1. Edit `setup.sh` and update:
   - `PROJECT_ID`
   - `BUCKET_NAME`
   - `DATASET`
   - `TABLE`
   - `REGION`
   - `UNIQUE_KEY`


2. Run the script:

```bash
. ./setup.sh
```

3. Edit `run_dataproc.py` and update
   - `PROJECT_ID`
   - `UNIQUE_KEY`

4. Run the script to craete dataproc cluster and submit job

```
python3 run_dataproc.py
```